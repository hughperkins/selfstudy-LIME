{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME\n",
    "\n",
    "LIME is 'locally interpetable model-agnostic explanations'.\n",
    "\n",
    "The paper is at http://arxiv.org/pdf/1602.04938v1.pdf , by Ribeiro, Singh, and Guestrin.  Ribeiro has a blog post about it at https://homes.cs.washington.edu/~marcotcr/blog/lime/ . There is code provided by Ribeiro at https://github.com/marcotcr/lime\n",
    "\n",
    "There is thus already ample documentation and code about LIME, and this repo is for self-study purposes primarily, and likely wont introduce anything much new to the world, for now :-)\n",
    "\n",
    "## What LIME does\n",
    "\n",
    "LIME does the following:\n",
    "- creates interpretable features, which for sparse nlp models means, bag of unigram words, containing all the words in the training vocabulary (I think)\n",
    "- samples from interpretable feature space, near an example we wish to explain\n",
    "- uses local gradients, from near the target example, to explain which interpretable features most affect decisions around that example\n",
    "\n",
    "## LIME Experiments\n",
    "\n",
    "### Train and test distributions differ\n",
    "\n",
    "- train on `news20`, for atheist vs christianity\n",
    "- test against new [religion](https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz) dataset, created from websites from from [DMOZ](https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz) directory\n",
    "  - these data points have similar classes to the news20 training sets, ie atheism vs christianity.  However, the features are fairly different, and eg learning the names of prolific atheist posters in news20 wont generalize to the DMOZ websites.\n",
    "- the idea is to examine to what extent the LIME explanations (or any other explanations for that matter) can facilitate rmeoving 'junk' features, after/during training, and thus improving the score on the DMOZ-derived dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data, and training simple linear model\n",
    "\n",
    "Let's start by downloading the datasets, and training a simple linear model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading religion dataset to memory...\n",
      "class_id_by_name {'christianity': 1, 'atheism': 0}\n",
      "failed to decode to utf-8 => skipping 1 doc\n",
      "... religion dataset loaded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# This is in a python script in same directory:\n",
    "from religion_dataset import fetch_religion\n",
    "\n",
    "\n",
    "global_categories = ['atheism', 'religion']\n",
    "news_categories = ['alt.atheism', 'soc.religion.christian']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=news_categories, shuffle=True, random_state=123)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=news_categories, shuffle=True, random_state=123)\n",
    "religion_test = fetch_religion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer: rbf\n",
      "twenty train: 100.0\n",
      "twenty test: 93.8633193863\n",
      "religion test: 56.8376068376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, trainer):\n",
    "        self.trainer = trainer\n",
    "        trainers = {\n",
    "            'nb': MultinomialNB(),\n",
    "            'sgd': SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                 alpha=1e-3, n_iter=5, random_state=123),\n",
    "            'rbf': SVC(C=1000000, kernel='rbf')\n",
    "        }\n",
    "        self.model = trainers[trainer]\n",
    "        print('trainer: %s' % trainer)\n",
    "\n",
    "    def train(self, bunch):\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.X_train_counts = self.count_vect.fit_transform(bunch.data)\n",
    "\n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        self.X_train_tfidf = self.tfidf_transformer.fit_transform(self.X_train_counts)\n",
    "\n",
    "        self.model.fit(self.X_train_tfidf, bunch.target)\n",
    "        train_pred = self.model.predict(self.X_train_tfidf)\n",
    "        train_num_right = np.equal(train_pred, bunch.target).sum()\n",
    "        return train_num_right / len(bunch.target) * 100\n",
    "\n",
    "    def confidence_from_counts(self, counts):\n",
    "        tfidf = self.tfidf_transformer.transform(counts)\n",
    "        confidence = self.model.decision_function(tfidf)\n",
    "        return confidence\n",
    "    \n",
    "    def test(self, bunch):\n",
    "        X_test_counts = self.count_vect.transform(bunch.data)\n",
    "\n",
    "        X_test_tfidf = self.tfidf_transformer.transform(X_test_counts)\n",
    "        test_pred = self.model.predict(X_test_tfidf)\n",
    "        test_num_right = np.equal(test_pred, bunch.target).sum()\n",
    "        return test_num_right / len(bunch.target) * 100\n",
    "\n",
    "model = Model('rbf')\n",
    "\n",
    "print('twenty train:', model.train(twenty_train))\n",
    "print('twenty test:', model.test(twenty_test))\n",
    "print('religion test:', model.test(religion_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model gets almost perfect accuracy on `twenty-news` training data, fairly good accuracy on `twenty-news` test data, and, as per the LIME paper, less good accuracy on the `religion` test data.  The model doesnt generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME Model\n",
    "\n",
    "### LIME Generalized Model\n",
    "\n",
    "LIME trains a model $\\xi$, drawn from a class $G$ of interpretable models.  Where interpretable models for LIME means simple-ish linear models, such as linear models, decision trees, or falling rule lists.  $\\xi$ is the solution to:\n",
    "\n",
    "$$\\xi(x) = \\mathrm{argmin}_{g \\in G} \\left( \\mathcal{L}(f, g,\\Pi_x)+\\Omega(g) \\right)$$\n",
    "\n",
    "Where:\n",
    "- $G$ is class of interpretable models\n",
    "- $f$ is the function learned by the network we wish to interpret\n",
    "- $\\Pi_x(z)$ is a measure of proximity of $z$ to $x$\n",
    "- $\\mathcal{L}$ is a measure of how unfaithful $g$ is in representing $f$ in the locality defined by $\\Pi(x)$\n",
    "- $\\Omega(\\cdot)$ is a measure of complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME Specialized Model\n",
    "\n",
    "This is a general formulation. For LIME, we add additional constraints and assumptions\n",
    "- $G$ is taken to be the class of linear models, and in particular: $g(z') = w_g \\cdot z'$, where $w_g$ are parameters to be learned\n",
    "- $\\Pi(x)$ is defined as: $\\exp \\left( \\frac{ -D(x, z)^2 } {\\sigma^2} \\right)$, so it's something like a radial basis function, and is close to one near $x$, then falls off with distance\n",
    "- $\\mathcal{L}$ is the square loss, weighted by locality:\n",
    "\n",
    "$$\\mathcal{L}(f, g, \\Pi_x) = \\sum_{z, z', \\mathcal{Z}} \\Pi_x(z) \\left( f(z) - g(z') \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable features for NLP\n",
    "\n",
    "The locally interpretable features are binary, $\\mathbf{x}' \\in \\{0,1\\}^{d'}$. ~~for nlp, LIME uses LARS to obtain the $K$ most important features/words from the model.  I think.  I think these interpretable features are global.  Again, I'm not entirely 100% sure on this point currently :-)~~\n",
    "\n",
    "For nlp, I think that the interpretable features are a bag of unigrams.  The unigrams includes all the entire vocabulary, I think.  Samples are drawn from this (presumably by perturbing the original example-to-be-explained slightly), then LARS path is run against these samples, to obtain the top $K$ explainers. I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing samples\n",
    "\n",
    "So, first we should draw samples.  I'm doubling the number of perturbations to each sample until the similarity drops below 0.1.  Its not ideal since the number of perturbations should probably be more like a probability, or percentage, than an absolute number, otherwise it will not handle varying document lengths very well.  But it's a start.\n",
    "\n",
    "Once we have the number of perturbations. We sample a number of perturbations up to that, for each document, then replace this number of features in the example with some random features.\n",
    "\n",
    "We should probably draw samples of total number of features in each sample, rather than keeping this fixed, and just swapping the features around.  For now though, swapping features around seems like a good start.\n",
    "\n",
    "Using classification as the result of $f$ didnt work very well for me: all the perturbed samples systematically had the same class as the original class.  However, by using the confidence score, instead of the prediced class, the samples seem to have reasonable diversity of confidence scores, albeit without sufficient diversity to change class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0 calib True num_perturbations 1\n",
      "n 1 calib True num_perturbations 2\n",
      "n 2 calib True num_perturbations 4\n",
      "n 3 calib True num_perturbations 8\n",
      "n 4 calib True num_perturbations 16\n",
      "n 5 calib True num_perturbations 32\n",
      "n 6 calib True num_perturbations 64\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "sampling done\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "\n",
    "N = 15000   # number of samples, from section 5.1 of the paper\n",
    "N = 5000\n",
    "rho = 25   # from https://github.com/marcotcr/lime-experiments/blob/master/generate_data_for_compare_classifiers.py#L62\n",
    "# distance is calculated as per https://github.com/marcotcr/lime-experiments/blob/master/explainers.py#L115:\n",
    "\"\"\"\n",
    "distance_fn = lambda x : sklearn.metrics.pairwise.cosine_distances(x[0],x)[0] * 100\n",
    "\"\"\"\n",
    "\n",
    "def sample_around(x, X, y, N, f, similarity_kernel, ifspace_to_inputspace_fn):\n",
    "    \"\"\"\n",
    "    For doing the perturbations, we will double the number of perturbations each sample, until\n",
    "    the effect of the distance_fn\n",
    "    \"\"\"\n",
    "    example_list = list(x.nonzero()[1])\n",
    "    active_set = set(x.nonzero()[1])\n",
    "    n = 0\n",
    "    samples = []\n",
    "    num_perturbations = 1\n",
    "    num_features = X.shape[1]\n",
    "    active_set_size = len(active_set)\n",
    "    z_example = csr_matrix((\n",
    "        [1] * active_set_size,\n",
    "        ([0] * active_set_size, example_list)\n",
    "    ), shape=(1, num_features))\n",
    "    calibrating_perturbations = True\n",
    "    while n < N:\n",
    "        if calibrating_perturbations:\n",
    "            print('n', n, 'calib', calibrating_perturbations, 'num_perturbations', num_perturbations)\n",
    "            new_list = list(active_set)\n",
    "            active_set_idxs = np.random.choice(active_set_size, size=(num_perturbations,))\n",
    "            swap_idxs = np.random.choice(num_features, size=(num_perturbations,))\n",
    "            for i, active_set_idx in enumerate(active_set_idxs):\n",
    "                new_list[active_set_idx] = swap_idxs[i]\n",
    "            z_ifspace = csr_matrix((\n",
    "                [1] * active_set_size,\n",
    "                ([0] * active_set_size, new_list)\n",
    "            ), shape=(1, num_features))\n",
    "            similarity = similarity_kernel(z_example, z_ifspace)\n",
    "            if similarity > 0.1:\n",
    "                num_perturbations *= 2\n",
    "                if num_perturbations > active_set_size:\n",
    "                    num_perturbations = active_set_size\n",
    "                    calibrating_perturbations = False\n",
    "            else:\n",
    "                calibrating_perturbations = False\n",
    "        else:\n",
    "            this_num_perturbations = random.randint(1, num_perturbations)\n",
    "            new_list = list(active_set)\n",
    "            active_set_idxs = np.random.choice(active_set_size, size=(this_num_perturbations,))\n",
    "            swap_idxs = np.random.choice(num_features, size=(this_num_perturbations,))\n",
    "            for i, active_set_idx in enumerate(active_set_idxs):\n",
    "                new_list[active_set_idx] = swap_idxs[i]\n",
    "            z_ifspace = csr_matrix((\n",
    "                [1] * active_set_size,\n",
    "                ([0] * active_set_size, new_list)\n",
    "            ), shape=(1, num_features))\n",
    "            similarity = similarity_kernel(z_example, z_ifspace)\n",
    "        z_inputspace = ifspace_to_inputspace_fn(z_ifspace)\n",
    "        pred = f(z_inputspace)[0]\n",
    "        sample = {\n",
    "            'similarity': similarity,\n",
    "            'z_ifspace': z_ifspace,\n",
    "            'z_inputspace': z_inputspace,\n",
    "            'pred': pred\n",
    "        }\n",
    "        samples.append(sample)\n",
    "        if (n + 1) % 1000 == 0:\n",
    "            print(n + 1)\n",
    "        n += 1\n",
    "    return samples\n",
    "\n",
    "def my_cosine_distance(v1, v2):\n",
    "    \"\"\"\n",
    "    As per https://github.com/marcotcr/lime-experiments/blob/master/explainers.py#L115\n",
    "    \"\"\"\n",
    "    return cosine_distances(v1, v2)[0][0] * 100\n",
    "\n",
    "def create_rbf_similarity_kernel(sigma, distance_fn):\n",
    "    sigma_squared = sigma * sigma\n",
    "    def my_similarity_kernel(v1, v2):\n",
    "        d = distance_fn(v1, v2)\n",
    "        return np.exp(-d * d / sigma_squared)\n",
    "    return my_similarity_kernel\n",
    "\n",
    "def tfidf_ifspace_to_inputspace_fn(z_ifspace):\n",
    "    z_inputspace = model.tfidf_transformer.transform(z_ifspace)\n",
    "    return z_inputspace\n",
    "\n",
    "my_similarity_kernel = create_rbf_similarity_kernel(sigma=rho, distance_fn=my_cosine_distance)\n",
    "samples = sample_around(\n",
    "    x=model.X_train_tfidf[2], X=model.X_train_tfidf, y=twenty_train.target, N=N,\n",
    "    f=model.confidence_from_counts,\n",
    "    similarity_kernel=my_similarity_kernel,\n",
    "    ifspace_to_inputspace_fn=tfidf_ifspace_to_inputspace_fn)\n",
    "print('sampling done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Lasso Lars\n",
    "\n",
    "For LIME, we need to run through Lasso LARS path, for the loss function stated above, ie:\n",
    "\n",
    "$$\\mathcal{L}(f, g, \\Pi_x) = \\sum_{z, z', \\mathcal{Z}} \\Pi_x(z) \\left( f(z) - g(z') \\right)^2 $$\n",
    "\n",
    "The proximity function is weighting the samples in the loss function.  Otherwise this is standard rms-loss linear regression.  Looking at sklearn library, [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) allows one to provide `sample_weight`s, however, [LassoLars](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars), which is what we want to use, does not.  We could implement Lasso Lars by hand, but I imagine there are lots of little tweaks and tricks inside the sklearn implementation, so I'm just going to wrap the sklearn one.\n",
    "\n",
    "Let's examine a general weighted mse-loss linear regression function:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_n (y_n - f(x_n))^2 $$\n",
    "\n",
    "If we have per-sample weights, $\\phi_n$, then this becomes:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_n \\phi_n(y_n - f(x_n))^2 $$\n",
    "\n",
    "We can move the $\\phi_n$ inside:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_n ( y_n\\sqrt{\\phi_n} - f(x_n)\\sqrt{\\phi_n})^2 $$\n",
    "\n",
    "... and note that $f(x_n)$ is $\\mathbf{w} \\cdot \\mathbf{x}_n$, for parameter vector $\\mathbf{w}$, to be learned.  So we can write:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_n (y_n \\sqrt{\\phi_n} - \\sum_j( w_j (x_{n,j}\\sqrt{\\phi_n})))^2 $$\n",
    "\n",
    "So, we simply need to scale the $\\mathbf{x}_n$ and $y_n$ values appropriately, by $\\sqrt{\\phi_n}$.  The only thing to be a bit careful of is that the analysis above doesnt include a bias/intercept.  However, by default both `LinearRegression` and `LassoLars` regression do, in the sklearn implementation, and it's so by default.  So, we'll need to handle the intercept ourselves, by adding an additional all-1s feature, and turn off the intercept in `LassoLars`.\n",
    "\n",
    "Writing as a class this gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class WeightedLassoLars(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.fit_intercept = kwargs.get('fit_intercept', True)\n",
    "        kwargs['fit_intercept'] = False\n",
    "        self.lasso = linear_model.LassoLars(**kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_intercept_feature(X):\n",
    "        # add intercept feature, all 1s\n",
    "        N = X.shape[0]\n",
    "        K = X.shape[1]\n",
    "#         X_new = np.zeros((N, K + 1), dtype=np.float32)\n",
    "        X_new = lil_matrix((N, K + 1), dtype=np.float32)\n",
    "        X_new[:, 1:] = X\n",
    "        X_new[:, 0] = 1\n",
    "        return X_new\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept_feature(X)\n",
    "        if sample_weight is not None:\n",
    "            for n, weight in enumerate(sample_weight):\n",
    "                X[n] *= math.sqrt(weight)\n",
    "                y[n] *= math.sqrt(weight)\n",
    "        self.lasso.fit(X.toarray(), y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept_feature(X)\n",
    "        return self.lasso.predict(X.toarray())\n",
    "    \n",
    "    @property\n",
    "    def active_(self):\n",
    "        res = []\n",
    "        for j in self.lasso.active_:\n",
    "            if j > 0:\n",
    "                res.append(j - 1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Lasso Lars for our samples\n",
    "\n",
    "So, now we just have to plug the sampled data into the new WeightedLassoLars class.  In theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso fitted\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "\n",
    "K = 10  # I *think* the paper uses K as 10\n",
    "\n",
    "lasso = WeightedLassoLars(alpha=0, max_iter=K + 1)  # +1, because intercept gets chosen first..\n",
    "samples_N = len(samples)\n",
    "ifspace_num_features = samples[0]['z_ifspace'].shape[1]\n",
    "samples_X_ifspace = lil_matrix((samples_N, ifspace_num_features), dtype=np.float32)\n",
    "samples_y = np.zeros((samples_N,), dtype=np.float32)\n",
    "samples_weights = np.zeros((samples_N,), dtype=np.float32)\n",
    "for n, sample in enumerate(samples):\n",
    "    samples_X_ifspace[n] = sample['z_ifspace']\n",
    "    samples_y[n] = sample['pred']\n",
    "    samples_weights[n] = sample['similarity']\n",
    "lasso.fit(samples_X_ifspace, samples_y, samples_weights)\n",
    "print('lasso fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the active features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11546, 11430, 12546, 10640, 10155, 15469, 3410, 4478, 6739, 11706]\n",
      "mathew\n",
      "mantis\n",
      "nntp\n",
      "kmr4\n",
      "isc\n",
      "rit\n",
      "bobby\n",
      "co\n",
      "eg\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "print(lasso.active_)\n",
    "for j in lasso.active_:\n",
    "    print(model.count_vect.get_feature_names()[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
