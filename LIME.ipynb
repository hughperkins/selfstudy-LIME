{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME\n",
    "\n",
    "LIME is 'locally interpetable model-agnostic explanations'.\n",
    "\n",
    "The paper is at http://arxiv.org/pdf/1602.04938v1.pdf , by Ribeiro, Singh, and Guestrin.  Ribeiro has a blog post about it at https://homes.cs.washington.edu/~marcotcr/blog/lime/ . There is code provided by Ribeiro at https://github.com/marcotcr/lime\n",
    "\n",
    "There is thus already ample documentation and code about LIME, and this repo is for self-study purposes primarily, and likely wont introduce anything much new to the world, for now :-)\n",
    "\n",
    "## What LIME does\n",
    "\n",
    "LIME does the following:\n",
    "- creates interpretable features, which for sparse nlp models means, they draw the first few features from a LARS path, and use those.  My own self-study notebook for LARS: https://github.com/hughperkins/selfstudy-LARS/blob/master/test_lars.ipynb\n",
    "- samples from interpretable feature space, near an example we wish to explain\n",
    "- uses local gradients, from near the target example, to explain which interpretable features most affect decisions around that example\n",
    "\n",
    "## LIME Experiments\n",
    "\n",
    "### Train and test distributions differ\n",
    "\n",
    "- train on `news20`, for atheist vs christianity\n",
    "- test against new [religion](https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz) dataset, created from websites from from [DMOZ](https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz) directory\n",
    "  - these data points have similar classes to the news20 training sets, ie atheism vs christianity.  However, the features are fairly different, and eg learning the names of prolific atheist posters in news20 wont generalize to the DMOZ websites.\n",
    "- the idea is to examine to what extent the LIME explanations (or any other explanations for that matter) can facilitate rmeoving 'junk' features, after/during training, and thus improving the score on the DMOZ-derived dataset\n",
    "\n",
    "Let's start by downloading the datasets, and training a simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer: sgd\n",
      "train 1078 99.9073215941\n",
      "test 670 93.4449093445\n",
      "loading religion dataset to memory...\n",
      "class_id_by_name {'christianity': 1, 'atheism': 0}\n",
      "failed to decode to utf-8 => skipping 1 doc\n",
      "... religion dataset loaded\n",
      "religion test 884 53.9682539683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import tarfile\n",
    "import sklearn.datasets\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "import argparse\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from os import path\n",
    "from os.path import join\n",
    "import urllib.request\n",
    "import hashlib\n",
    "\n",
    "\n",
    "global_categories = ['atheism', 'religion']\n",
    "news_categories = ['alt.atheism', 'soc.religion.christian']\n",
    "\n",
    "religion_url = 'https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz?raw=true'\n",
    "\n",
    "\n",
    "def get_md5(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        dat = f.read()\n",
    "    return hashlib.md5(dat).hexdigest()\n",
    "\n",
    "\n",
    "def download_religion():\n",
    "    file_exists = False\n",
    "    home_dir = os.environ['HOME']\n",
    "    limedata_dir = join(home_dir, 'limedata')\n",
    "    if not path.isdir(limedata_dir):\n",
    "        os.makedirs(limedata_dir)\n",
    "    religion_filepath = join(limedata_dir, 'religion_dataset.tar.gz')\n",
    "    if path.isfile(religion_filepath):\n",
    "        md5sum = get_md5(religion_filepath)\n",
    "        if md5sum == '0f12beb283869a09584493ddf93672b6':\n",
    "            file_exists = True\n",
    "    if not file_exists:\n",
    "        print('downloading religion dataset...')\n",
    "        with urllib.request.urlopen(religion_url) as response, open(religion_filepath, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "            print(get_md5(religion_filepath))\n",
    "            print('... downloaded religion dataset')\n",
    "    return religion_filepath\n",
    "\n",
    "\n",
    "def fetch_religion():\n",
    "    tar_filepath = download_religion()\n",
    "    # res = sklearn.datasets.base.Bunch()\n",
    "    examples = []\n",
    "    print('loading religion dataset to memory...')\n",
    "    tar = tarfile.open(tar_filepath)\n",
    "    # print(tar.getmembers())\n",
    "    class_name_by_id = ['atheism', 'christianity']\n",
    "    class_id_by_name = {name: id for id, name in enumerate(class_name_by_id)}\n",
    "    print('class_id_by_name', class_id_by_name)\n",
    "    N_per_class = 819\n",
    "    y = np.zeros((N_per_class * 2), dtype=np.int64)\n",
    "    n = 0\n",
    "    count_per_class = defaultdict(int)\n",
    "    for m in tar.getmembers():\n",
    "        # print(m)\n",
    "        # print(dir(m))\n",
    "        # print(m.name, m.path, m.type)\n",
    "        if '/' in m.path:\n",
    "            class_name = m.path.split('/')[0]\n",
    "            class_id = class_id_by_name[class_name]\n",
    "            if count_per_class[class_id] >= N_per_class:\n",
    "                continue\n",
    "            # if m.path not in ['README.txt', 'atheism', 'christianity']:\n",
    "            f = tar.extractfile(m)\n",
    "            try:\n",
    "                content = f.read()\n",
    "                content = content.decode('utf-8')\n",
    "            except:\n",
    "                # raise Exception('failed for [%s]' % content)\n",
    "                print('failed to decode to utf-8 => skipping 1 doc')\n",
    "                continue\n",
    "            finally:\n",
    "                f.close()\n",
    "            examples.append(content)\n",
    "            y[n] = class_id\n",
    "            count_per_class[class_id] += 1\n",
    "            n += 1\n",
    "    tar.close()\n",
    "    print('... religion dataset loaded')\n",
    "    return sklearn.datasets.base.Bunch(data=examples, target=y)\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, trainer):\n",
    "        self.trainer = trainer\n",
    "        trainers = {\n",
    "            'nb': MultinomialNB(),\n",
    "            'sgd': SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                 alpha=1e-3, n_iter=5, random_state=123),\n",
    "            'rbf': SVC(C=1000000, kernel='rbf')\n",
    "        }\n",
    "        self.model = trainers[trainer]\n",
    "        print('trainer: %s' % trainer)\n",
    "\n",
    "    def train(self):\n",
    "        self.twenty_train = fetch_20newsgroups(subset='train', categories=news_categories, shuffle=True, random_state=123)\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.X_train_counts = self.count_vect.fit_transform(self.twenty_train.data)\n",
    "\n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        self.X_train_tfidf = self.tfidf_transformer.fit_transform(self.X_train_counts)\n",
    "\n",
    "        # model = MultinomialNB()\n",
    "        self.model.fit(self.X_train_tfidf, self.twenty_train.target)\n",
    "        train_pred = self.model.predict(self.X_train_tfidf)\n",
    "        train_num_right = np.equal(train_pred, self.twenty_train.target).sum()\n",
    "        print('train', train_num_right, train_num_right / len(self.twenty_train.target) * 100)\n",
    "        # return model\n",
    "\n",
    "    def test(self):\n",
    "        self.twenty_test = fetch_20newsgroups(subset='test', categories=news_categories, shuffle=True, random_state=123)\n",
    "        X_test_counts = self.count_vect.transform(self.twenty_test.data)\n",
    "\n",
    "        X_test_tfidf = self.tfidf_transformer.transform(X_test_counts)\n",
    "        test_pred = self.model.predict(X_test_tfidf)\n",
    "        test_num_right = np.equal(test_pred, self.twenty_test.target).sum()\n",
    "        print('test', test_num_right, test_num_right / len(self.twenty_test.target) * 100)\n",
    "\n",
    "        # now try religion dataset, from https://github.com/marcotcr/lime-experiments/blob/master/religion_dataset.tar.gz\n",
    "        religion_test = fetch_religion()\n",
    "        religion_X_test_counts = self.count_vect.transform(religion_test.data)\n",
    "        religion_X_test_tfidf = self.tfidf_transformer.transform(religion_X_test_counts)\n",
    "        religion_test_pred = self.model.predict(religion_X_test_tfidf)\n",
    "        religion_test_num_right = np.equal(religion_test_pred, religion_test.target).sum()\n",
    "        print('religion test', religion_test_num_right, religion_test_num_right / len(religion_test.target) * 100)\n",
    "\n",
    "model = Model('sgd')\n",
    "model.train()\n",
    "model.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME trains a model $\\xi$, drawn from a class $G$ of interpretable models.  Where interpretable models for LIME means simple-ish linear models, such as linear models, decision trees, or falling rule lists.  $\\xi$ is the solution to:\n",
    "\n",
    "$$\\xi(x) = \\mathrm{argmin}_{g \\in G} \\left( \\mathcal{L}(f, g,\\Pi_x)+\\Omega(g) \\right)$$\n",
    "\n",
    "Where:\n",
    "- $G$ is class of interpretable models\n",
    "- $f$ is the function learned by the network we wish to interpret\n",
    "- $\\Pi_x(z)$ is a measure of proximity of $z$ to $x$\n",
    "- $\\mathcal{L}$ is a measure of how unfaithful $g$ is in representing $f$ in the locality defined by $\\Pi(x)$\n",
    "- $\\Omega(\\cdot)$ is a measure of complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a general formulation. For LIME, we add additional constraints and assumptions\n",
    "- $G$ is taken to be the class of linear models, and in particular: $g(z') = w_g \\cdot z'$, where $w_g$ are parameters to be learned\n",
    "- $\\Pi(x)$ is defined as: $\\exp \\left( \\frac{ -D(x, z)^2 } {\\sigma^2} \\right)$, so it's something like a radial basis function, and is close to one near $x$, then falls off with distance\n",
    "- $\\mathcal{L}$ is the square loss, weighted by locality:\n",
    "\n",
    "$$\\mathcal{L}(f, g, \\Pi_x) = \\sum_{z, z', \\mathcal{Z}} \\Pi_x(z) \\left( f(z) - g(z') \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locally interpretable features are binary, $\\mathbf{x}' \\in \\{0,1\\}^{d'}$. ~~for nlp, LIME uses LARS to obtain the $K$ most important features/words from the model.  I think.  I think these interpretable features are global.  Again, I'm not entirely 100% sure on this point currently :-)~~\n",
    "\n",
    "For nlp, I think that the interpretable features are a bag of unigrams.  The unigrams includes all the entire vocabulary, I think.  Samples are drawn from this (presumably by perturbing the original example-to-be-explained slightly), then LARS path is run against these samples, to obtain the top $K$ explainers, I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first we should draw samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "N = 15000   # number of samples, from section 5.1 of the paper\n",
    "K = 10  # I *think* the paper uses K as 10\n",
    "rho = 25   # from https://github.com/marcotcr/lime-experiments/blob/master/generate_data_for_compare_classifiers.py#L62\n",
    "# distance is calculated as per https://github.com/marcotcr/lime-experiments/blob/master/explainers.py#L115:\n",
    "\"\"\"\n",
    "distance_fn = lambda x : sklearn.metrics.pairwise.cosine_distances(x[0],x)[0] * 100\n",
    "\"\"\"\n",
    "\n",
    "def foo():\n",
    "    from sklearn import datasets\n",
    "    diabetes = datasets.load_diabetes()\n",
    "    X = diabetes.data\n",
    "    print(type(X), X.shape)\n",
    "    y = diabetes.target\n",
    "    print(type(y), y.shape)\n",
    "    alphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)\n",
    "foo()\n",
    "print('trained foo')\n",
    "\n",
    "print(type(model.X_train_tfidf))\n",
    "print(type(model.twenty_train.target))\n",
    "lasso_model = linear_model.LassoLars(alpha=0.001, max_iter=K)\n",
    "lasso_model.fit(model.X_train_tfidf.toarray(), model.twenty_train.target)\n",
    "# alphas, _, coefs = linear_model.lars_path(\n",
    "#     model.X_train_tfidf, model.twenty_train.target, method='lasso', verbose=True)\n",
    "print(lasso_model.coef_)\n",
    "print(lasso_model.active_)b\n",
    "# for j in lasso_model.active_:\n",
    "# print(model.count_vect.get_feature_names(lasso_model.active_))\n",
    "names = model.count_vect.get_feature_names()\n",
    "print(len(names.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(names[:5000])\n",
    "print(len(names))\n",
    "# print(names[lasso_model.active_])\n",
    "for j in lasso_model.active_:\n",
    "    print(names[j])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
